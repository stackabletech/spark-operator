= Usage

== Create an Apache Spark cluster

If you followed the installation instructions, you should now have a Stackable Operator for Apache Spark up and running and you are ready to create your first Apache Spark cluster.

The example below creates an Apache Spark 3.0.1 cluster with one master (because of `replicas: 1`), one history server and as many workers as nodes available.
Because of `enableMonitoring: true`, all pods will be annotated for metric scraping. Metrics are retrieved from Spark's built in Prometheus Servlet.

    cat <<EOF | kubectl apply -f -
    apiVersion: spark.stackable.tech/v1alpha1
    kind: SparkCluster
    metadata:
      name: simple
    spec:
      version: "3.0.1"
      config:
        logDir: /stackable/data/log
        enableMonitoring: true
      masters:
        roleGroups:
          default:
            selector:
              matchLabels:
                kubernetes.io/os: linux
            replicas: 1
            config:
              masterPort: 7078
              masterWebUiPort: 8081
      workers:
        roleGroups:
          2core2g:
            selector:
              matchLabels:
                kubernetes.io/os: linux
            replicas: 1
            config:
              cores: 2
              memory: "2g"
              workerPort: 3031
              workerWebUiPort: 8083
      historyServers:
        roleGroups:
          default:
            selector:
              matchLabels:
                kubernetes.io/os: linux
            replicas: 1
            config:
              historyWebUiPort: 18081
    EOF

== Using the Apache Spark cluster

The example above sets up a https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport[NodePort] service named `simple` that clients can use to run Spark jobs.

This is how the list of services might look like:

    kubectl get svc
    NAME                            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                         AGE
    kubernetes                      ClusterIP   10.96.0.1       <none>        443/TCP                         20m
    simple                          NodePort    10.96.220.230   <none>        7078:31585/TCP,8081:30497/TCP   31s
    simple-history-server-default   ClusterIP   None            <none>        18081/TCP                       31s
    simple-master-default           ClusterIP   None            <none>        7078/TCP,8081/TCP               31s
    simple-slave-2core2g            ClusterIP   None            <none>        3031/TCP,8083/TCP               31s

To start a Spark shell from inside the cluster you would run:

    /stackable/spark/bin/spark-shell --master spark://simple-master-default:7078

and from outside: 

    spark-shell --master spark://172.18.0.2:31585

where `172.18.0.2` is the IP address o one your nodes.

== Configuration properties

There are three levels of configuration:

[cols="1,1"]
|===
|Common properties
|Contains configuration that is shared within the whole cluster. For example the spark version, secrets, encryption or logging options.

|Role properties
|This configuration is shared for all nodes of a certain type (Master, Worker, History-Server)

|Role Group properties
|Options provided in the role group are limited to one specific role and role group.
|===

=== Common properties
[cols="1,1,1,1"]
|===
|Name
|Type
|Description
|Related Spark properties

|version
|string
|The spark version used in the format: x.y.z
|

|logDir
|string
|The log folder for spark applications
|spark.history.fs.logDirectory=logDir, spark.eventLog.enabled=true, spark.eventLog.dir=logDir;

|secret
|string
|A secret shared between nodes and required to submit applications via spark-submit
|spark.authenticate=true, spark.authenticate.secret=secret;

|maxPortRetries
|integer
|Maximum number of retries when binding to a port before giving up. When a port is given a specific value (non 0), each subsequent retry will increment the port used in the previous attempt by 1 before retrying. This essentially allows it to try a range of ports from the start port specified to port + maxRetries.
|spark.port.maxRetries
|===

=== Role properties
T.b.d.

=== Role Group properties
==== Master
[cols="1,1,1,1"]
|===
|Name
|Type
|Description
|Related Spark properties

|masterPort
|integer
|Start the master on a different port (default: 7077).
|SPARK_MASTER_PORT

|masterWebUiPort
|integer
|Port for the master web UI (default: 8080).
|SPARK_MASTER_WEBUI_PORT
|===
==== Worker
[cols="1,1,1,1"]
|===
|Name
|Type
|Description
|Related Spark properties

|workerPort
|integer
|Start the Spark worker on a specific port (default: random).
|SPARK_WORKER_PORT

|workerWebUiPort
|integer
|Port for the worker web UI (default: 8081).
|SPARK_WORKER_WEBUI_PORT

|cores
|integer
|Total number of cores to allow Spark jobs to use on the machine (default: all available cores).
|SPARK_WORKER_CORES

|memory
|string
|Total amount of memory to allow Spark jobs to use on the machine, e.g. 1000M, 2G (default: total memory minus 1 GB).
|SPARK_WORKER_MEMORY
|===

==== History Server
[cols="1,1,1,1"]
|===
|Name
|Type
|Description
|Related Spark properties

|storePath
|string
|A local directory where to cache application data. If set, the history server will store application data on disk instead of keeping it in memory. The data written to disk will be re-used in the event of a history server restart.
|spark.history.store.path

|historyUiPort
|integer
|The port to which the web interface of the history server binds (default: 18080).
|spark.history.ui.port
|===
