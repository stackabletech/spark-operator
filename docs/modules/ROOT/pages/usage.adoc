= Usage

The cluster can be configured via a YAML file.
Each role (Master, Worker, History-Server) can have multiple instances within multiple role groups.
Multiple instances in the same selector may lead to port duplicates which results in the affected node
to increase the port one by one (if max_port_retries is not set to 0). Keep that in mind when, for example running multiple masters without specified ports on the same node.

  apiVersion: spark.stackable.tech/v1
  kind: SparkCluster
  metadata:
    name: simple
  spec:
    version: "3.0.1"
    masters:
      roleGroups:
        default:
          selector:
            matchLabels:
              kubernetes.io/arch: stackable-linux
          replicas: 1
          config:
            masterPort: 7078
            masterWebUiPort: 8081
    workers:
      roleGroups:
        1core1g:
          selector:
            matchLabels:
              kubernetes.io/arch: stackable-linux
          replicas: 2
          config:
            cores: 1
            memory: "1g"
            workerPort: 3031
            workerWebUiPort: 8083
    historyServers:
      roleGroups:
        default:
          selector:
            matchLabels:
              kubernetes.io/arch: stackable-linux
          replicas: 1
          config:
            historyWebUiPort: 18081

=== Structure

There are three levels of configuration:

[cols="1,1"]
|===
|Common properties
|Contains configuration that is shared within the whole cluster. For example the spark version, secrets, encryption or logging options.

|Role properties
|This configuration is shared for all nodes of a certain type (Master, Worker, History-Server)

|Role Group properties
|Options provided in the role group are limited to one specific role and role group.
|===

=== Common properties
[cols="1,1,1,1"]
|===
|Name
|Type
|Description
|Related Spark properties

|version
|string
|The spark version used in the format: x.y.z
|

|logDir
|string
|The log folder for spark applications (must created by user)
|spark.history.fs.logDirectory=logDir, spark.eventLog.enabled=true, spark.eventLog.dir=logDir;

|secret
|string
|A secret shared between nodes and required to submit applications via spark-submit
|spark.authenticate=true, spark.authenticate.secret=secret;

|maxPortRetries
|integer
|Maximum number of retries when binding to a port before giving up. When a port is given a specific value (non 0), each subsequent retry will increment the port used in the previous attempt by 1 before retrying. This essentially allows it to try a range of ports from the start port specified to port + maxRetries.
|spark.port.maxRetries
|===

=== Role properties
T.b.d.

=== Role Group properties
==== Master
[cols="1,1,1,1"]
|===
|Name
|Type
|Description
|Related Spark properties

|masterPort
|integer
|Start the master on a different port (default: 7077).
|SPARK_MASTER_PORT

|masterWebUiPort
|integer
|Port for the master web UI (default: 8080).
|SPARK_MASTER_WEBUI_PORT
|===
==== Worker
[cols="1,1,1,1"]
|===
|Name
|Type
|Description
|Related Spark properties

|workerPort
|integer
|Start the Spark worker on a specific port (default: random).
|SPARK_WORKER_PORT

|workerWebUiPort
|integer
|Port for the worker web UI (default: 8081).
|SPARK_WORKER_WEBUI_PORT

|cores
|integer
|Total number of cores to allow Spark jobs to use on the machine (default: all available cores).
|SPARK_WORKER_CORES

|memory
|string
|Total amount of memory to allow Spark jobs to use on the machine, e.g. 1000M, 2G (default: total memory minus 1 GB).
|SPARK_WORKER_MEMORY
|===

==== History Server
[cols="1,1,1,1"]
|===
|Name
|Type
|Description
|Related Spark properties

|storePath
|string
|A local directory where to cache application data. If set, the history server will store application data on disk instead of keeping it in memory. The data written to disk will be re-used in the event of a history server restart.
|spark.history.store.path

|historyUiPort
|integer
|The port to which the web interface of the history server binds (default: 18080).
|spark.history.ui.port
|===
